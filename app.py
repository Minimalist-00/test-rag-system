import os
import re
import logging
from dotenv import load_dotenv
import streamlit as st
from openai import AzureOpenAI
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizedQuery

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()
search_endpoint = os.environ['SEARCH_ENDPOINT']
search_key = os.environ['SEARCH_API_KEY']
indexnametemp = os.environ['SEARCH_INDEX_NAME']
top_k_temp = 3  # æ¤œç´¢çµæœã®ä¸Šä½ä½•ä»¶ã‚’è¡¨ç¤ºã™ã‚‹ã‹

# APIè¨­å®šæƒ…å ±ã®ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
logger.info(f"OPENAI_API_ENDPOINT: {os.environ.get('OPENAI_API_ENDPOINT')}")
logger.info(f"EMBEDDING_API_ENDPOINT: {os.environ.get('EMBEDDING_API_ENDPOINT')}")
logger.info(f"EMBEDDING_MODEL: {os.environ.get('EMBEDDING_MODEL')}")

# ä¼šè©±ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
chat_client = AzureOpenAI(
    api_key=os.environ['OPENAI_API_KEY'],
    api_version=os.environ['OPENAI_API_VERSION'],
    azure_endpoint=os.environ['OPENAI_API_ENDPOINT']
)

# åŸ‹ã‚è¾¼ã¿ç”Ÿæˆç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ - å…±é€šã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
embedding_client = AzureOpenAI(
    api_key=os.environ.get('EMBEDDING_API_KEY', os.environ['OPENAI_API_KEY']),
    api_version=os.environ['OPENAI_API_VERSION'],
    azure_endpoint=os.environ.get('EMBEDDING_API_ENDPOINT', os.environ['OPENAI_API_ENDPOINT'])
)

openai_engine = os.environ['OPENAI_ENGINE']
openai_embedding_model = os.environ['EMBEDDING_MODEL']

# ã‚¿ã‚¤ãƒˆãƒ«ã‚„æœ¬æ–‡ã€ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def generate_embeddings(text, text_limit=7000):
    # ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ï¼ˆæ”¹è¡Œã‚„ç©ºç™½ã‚’å‰Šé™¤ï¼‰
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'[\n\r]+', ' ', text).strip()
    if len(text) > text_limit:
        logging.warning("ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒä¸Šé™ã‚’è¶…ãˆãŸãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šæ¨ã¦ã¾ã™ã€‚")
        text = text[:text_limit]

    try:
        logger.info(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: ãƒ¢ãƒ‡ãƒ«={openai_embedding_model}, ãƒ†ã‚­ã‚¹ãƒˆé•·={len(text)}")
        response = embedding_client.embeddings.create(input=text, model=openai_embedding_model)
        embeddings = response.data[0].embedding
        return embeddings
    except Exception as e:
        logger.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾ã—ã¦ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
def query_vector_index(query, searchtype, top_k_parameter, search_client):
    try:
        vector = generate_embeddings(query)
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¨ãã¯ã€search_textã‚’Noneã«ã™ã‚‹
        if searchtype == "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢":
            search_text = None
        else:
            search_text = query

        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¾ãŸã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å ´åˆ
        if searchtype == "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢" or searchtype == "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢":
            results = search_client.search(
                search_text=search_text,
                vector_queries=[
                    VectorizedQuery(
                        kind="vector",
                        vector=vector,
                        k_nearest_neighbors=top_k_parameter,
                        fields="text_vector"
                    )
                ],
            )
        # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã®å ´åˆ
        else:
            results = search_client.search(search_text=search_text, top=top_k_parameter)

        return results
    except Exception as e:
        logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return []

def main():
    # ãƒšãƒ¼ã‚¸ã®è¨­å®š
    st.set_page_config(page_title="RAG Sample Application", page_icon="ğŸ’¬", layout="wide")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state['messages'] = []

    # ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
    if st.sidebar.button("Clear Chat"):
        st.session_state['messages'] = []
        promptall = ""

    # æ¤œç´¢é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    st.sidebar.markdown("### Azure AI Search é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    top_k_parameter = st.sidebar.text_input("æ¤œç´¢çµæœå¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°", top_k_temp)
    indexname = st.sidebar.text_input("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å", indexnametemp)
    search_type = st.sidebar.radio("æ¤œç´¢æ–¹æ³•", ("ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢"))

    # ChatGPTé–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    st.sidebar.markdown("### Azure OpenAI é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    Temperature_temp = st.sidebar.slider("Temperature", 0.0, 1.0, 0.0, 0.01)

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ­ãƒ¼ãƒ«ã®å®šç¾©
    SystemRole = st.sidebar.text_area("System Role",
"""###å‰ææ¡ä»¶
ã‚ãªãŸã¯ã€ãƒŠãƒ¬ãƒƒã‚¸ã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¯¾ã™ã‚‹è³ªå•ã‚’ã™ã‚‹éš›ã«æ”¯æ´ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
###åˆ¶ç´„
ãƒ»å›ç­”ã«ã¯å½¹å‰²(userã‚„assistantãªã©)ã®æƒ…å ±ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
ãƒ»Sources(æƒ…å ±æº)ã«ãƒªã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹äº‹å®Ÿã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
ãƒ»ååˆ†ãªæƒ…å ±ãŒãªã„å ´åˆã¯ã€ã‚ã‹ã‚‰ãªã„ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
ãƒ»Sources(æƒ…å ±æº)ã‚’ä½¿ç”¨ã—ãªã„å›ç­”ã¯ç”Ÿæˆã—ãªã„ã§ãã ã•ã„ã€‚
ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®è³ªå•ã«ã‚ˆã£ã¦æ˜ç¢ºåŒ–ãŒå¿…è¦ãªå ´åˆã¯ã€è³ªå•ã—ã¦ãã ã•ã„ã€‚""")

    if SystemRole:
        # æ—¢ã«systemãƒ­ãƒ¼ãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ç¢ºèªã—ã€ãªã‘ã‚Œã°è¿½åŠ 
        if not any(message["role"] == "system" for message in st.session_state.messages):
            st.session_state.messages.append({"role": "system", "content": SystemRole})

    # æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç”Ÿæˆ
    credential = AzureKeyCredential(search_key)
    search_client = SearchClient(
        endpoint=search_endpoint,
        index_name=indexname,
        credential=credential
    )

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤ºï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ï¼‰
    with st.sidebar.expander("ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
        st.text(f"Embedding Model: {openai_embedding_model}")
        st.text(f"API Endpoint: {os.environ.get('EMBEDDING_API_ENDPOINT', os.environ['OPENAI_API_ENDPOINT'])}")
        st.text(f"API Version: {os.environ['OPENAI_API_VERSION']}")

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€æ¤œç´¢ã¨å¿œç­”ç”Ÿæˆã‚’è¡Œã†
    if user_input := st.chat_input("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        try:
            results = query_vector_index(user_input, search_type, top_k_parameter, search_client)

            prompt_source = ""
            for result in results:
                Score = result['@search.score']
                filepath = result['title']
                chunk_id = re.search(r'(?<=pages_).*', result['chunk_id']).group(0)
                content = result['chunk']

                prompt_source += f"#filepath: {filepath}\n\n#chunk_id: {chunk_id}\n\n#score: {Score}\n\n#content: {content}\n\n"

            promptall = "###Soruces(æƒ…å ±æº): \n\n" + prompt_source + "###è³ªå•ï¼š \n\n" + user_input
            message_temp = st.session_state.messages + [{"role": "user", "content": promptall}]

            with st.sidebar.expander("æ¤œç´¢çµæœã®è¡¨ç¤º"):
                st.markdown(prompt_source)

            with st.spinner("ChatGPTãŒå›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™"):
                output = chat_client.chat.completions.create(
                    model=openai_engine,
                    messages=message_temp,
                    temperature=Temperature_temp,
                    max_tokens=1000,
                    frequency_penalty=0,
                    presence_penalty=0,
                )

            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã¨AIå‡ºåŠ›ã‚’è¿½åŠ 
            st.session_state.messages.append({"role": "user", "content": user_input})
            st.session_state.messages.append({"role": "assistant", "content": output.choices[0].message.content})
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    messages = st.session_state.get('messages', [])
    for message in messages:
        if message['role'] == 'assistant':
            with st.chat_message('assistant'):
                st.markdown(message['content'])
        elif message['role'] == 'user':
            with st.chat_message('user'):
                st.markdown(message['content'])
        else:
            pass

if __name__ == '__main__':
    main()
