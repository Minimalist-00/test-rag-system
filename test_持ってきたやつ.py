import os
import re
import logging
from dotenv import load_dotenv
import streamlit as st
from openai import AzureOpenAI
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient  
from azure.search.documents.models import VectorizedQuery

load_dotenv()
search_endpoint = os.environ['SEARCH_ENDPOINT']
search_key = os.environ['SEARCH_API_KEY']
indexnametemp = os.environ['SEARCH_INDEX_NAME']
top_k_temp=3 #æ¤œç´¢çµæœã®ä¸Šä½ä½•ä»¶ã‚’è¡¨ç¤ºã™ã‚‹ã‹
client = AzureOpenAI(
  api_key=os.environ['OPENAI_API_KEY'],  
  api_version=os.environ['OPENAI_API_VERSION'],
  azure_endpoint = os.environ['OPENAI_API_ENDPOINT']
)

openai_engine = os.environ['OPENAI_ENGINE']
openai_embedding_model = os.environ['OPENAI_EMBEDDING_MODEL']

# Function to generate embeddings for title and content fields, also used for query embeddings
def generate_embeddings(text, text_limit=7000):
    # Clean up text (e.g. line breaks, )    
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'[\n\r]+', ' ', text).strip()
    if len(text) > text_limit:
        logging.warning("Token limit reached exceeded maximum length, truncating...")
        text = text[:text_limit]

    response = client.embeddings.create(input=text, model=openai_embedding_model)
    embeddings = response.data[0].embedding
    return embeddings
    
def query_vector_index(query, searchtype, top_k_parameter, search_client):
    vector = generate_embeddings(query)
    # æ¤œç´¢æ–¹æ³•ãŒãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å ´åˆã¯ã€search_textã‚’Noneã«ã™ã‚‹
    if searchtype == "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢":
        search_text = None
    # æ¤œç´¢æ–¹æ³•ãŒvector_onlyä»¥å¤–ã®å ´åˆã¯ã€search_textã«queryã‚’è¨­å®šã™ã‚‹
    else:
        search_text = query
    
    if searchtype == "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢" or searchtype == "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢":
        results = search_client.search(search_text=search_text, 
                                        vector_queries=[
                                            VectorizedQuery(
                                            kind="vector", vector=vector, k_nearest_neighbors=top_k_parameter, fields="text_vector"
                                            )
                                        ],)
    # æ¤œç´¢æ–¹æ³•ãŒãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã®å ´åˆ
    else:
        results = search_client.search(search_text=search_text, top=top_k_parameter)

    return results

def main():
    # Set page title and icon
    st.set_page_config(page_title="RAG Sample Application", page_icon="ğŸ’¬", layout="wide")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state['messages'] = []

    # ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸå ´åˆã€ãƒãƒ£ãƒƒãƒˆã¨st.text_input,promptallã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã€‚
    if st.sidebar.button("Clear Chat"):
        st.session_state['messages'] = []
        promptall = ""

    # Set Search parameters in sidebar
    st.sidebar.markdown("### Azure AI Search é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    # æ¤œç´¢çµæœã®ä¸Šä½ä½•ä»¶ã‚’å¯¾è±¡ã¨ã™ã‚‹ã‹ã‚’è¨­å®šã™ã‚‹ã€‚top_k_parameterã®è¨­å®šã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã§æŒ‡å®šã™ã‚‹ã€‚
    top_k_parameter = st.sidebar.text_input("æ¤œç´¢çµæœå¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°", top_k_temp)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã§æŒ‡å®šã™ã‚‹ã€‚indexnameã®è¨­å®š
    indexname = st.sidebar.text_input("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å", indexnametemp)

    # æ¤œç´¢æ–¹æ³•ã‚’é¸æŠã™ã‚‹ã€‚ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ or ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ or ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã€‚
    search_type = st.sidebar.radio("æ¤œç´¢æ–¹æ³•", ("ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢", "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢"))

    # Set ChatGPT parameters in sidebar
    st.sidebar.markdown("### Azure OpenAI é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    Temperature_temp = st.sidebar.slider("Temperature", 0.0, 1.0, 0.0, 0.01)

    # Define system role in text area
    SystemRole = st.sidebar.text_area("System Role",
"""###å‰ææ¡ä»¶
ã‚ãªãŸã¯ã€ãƒŠãƒ¬ãƒƒã‚¸ã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¯¾ã™ã‚‹è³ªå•ã‚’ã™ã‚‹éš›ã«æ”¯æ´ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
###åˆ¶ç´„
    ãƒ»å›ç­”ã«ã¯å½¹å‰²(userã‚„assistantãªã©)ã®æƒ…å ±ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
    ãƒ»Sources(æƒ…å ±æº)ã«ãƒªã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹äº‹å®Ÿã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
    ãƒ»ååˆ†ãªæƒ…å ±ãŒãªã„å ´åˆã¯ã€ã‚ã‹ã‚‰ãªã„ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
    ãƒ»Sources(æƒ…å ±æº)ã‚’ä½¿ç”¨ã—ãªã„å›ç­”ã¯ç”Ÿæˆã—ãªã„ã§ãã ã•ã„
    ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®è³ªå•ã«ã‚ˆã£ã¦æ˜ç¢ºåŒ–ãŒå¿…è¦ãªå ´åˆã¯ã€è³ªå•ã—ã¦ãã ã•ã„ã€‚""")

    # Add system role to session state
    if SystemRole:
        #æ—¢ã«roleãŒsystemã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã¯ã€è¿½åŠ ã—ãªã„ã€‚ãªã„å ´åˆã¯è¿½åŠ ã™ã‚‹ã€‚
        if not any(message["role"] == "system" for message in st.session_state.messages):
            st.session_state.messages.append({"role": "system", "content": SystemRole})

    #æ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹
    credential = AzureKeyCredential(search_key)
    search_client = SearchClient(endpoint=search_endpoint,
                                 index_name=indexname,
                                 credential=credential)

    # ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®å…¥åŠ›ã‚’å–å¾—ã™ã‚‹
    if user_input := st.chat_input("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        #æ¤œç´¢ã™ã‚‹ã€‚search_fieldsã¯contentã‚’å¯¾è±¡ã«æ¤œç´¢ã™ã‚‹
        results = query_vector_index(user_input, search_type, top_k_parameter, search_client)
            
        # å¤‰æ•°ã‚’åˆæœŸåŒ–ã™ã‚‹
        prompt_source = ""

        # resultsã‹ã‚‰å„resultã®çµæœã‚’å¤‰æ•°prompt_sourceã«ä»£å…¥ã™ã‚‹ã€‚filepathã¨contentã®æƒ…å ±ã‚’ä»£å…¥ã™ã‚‹ã€‚
        for result in results:
            Score = result['@search.score']
            filepath = result['title']
            chunk_id = re.search(r'(?<=pages_).*', result['chunk_id']).group(0)
            content = result['chunk']
    
            # å¤‰æ•°prompt_sourceã«å„å¤‰æ•°ã®å€¤ã‚’è¿½åŠ ã™ã‚‹
            prompt_source += f"#filepath: {filepath}\n\n  #chunk_id: {chunk_id}\n\n #score: {Score}\n\n #content: {content}\n\n"

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹
        promptall="###Soruces(æƒ…å ±æº): \n\n" + prompt_source + "###è³ªå•ï¼š \n\n" + user_input
        message_temp = []
        message_temp = st.session_state.messages + [{"role": "user", "content": promptall}]

         # expanderã‚’ä½œæˆã™ã‚‹
        with st.sidebar.expander("æ¤œç´¢çµæœã®è¡¨ç¤º"):
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹
            st.markdown(prompt_source)

        with st.spinner("ChatGPTãŒå›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™"):
            output =client.chat.completions.create(
                model=openai_engine,
                messages=message_temp,
                temperature=Temperature_temp,
                max_tokens=1000,
                frequency_penalty=0,
                presence_penalty=0,
            )
       
        # Add ChatGPT response to conversation
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.session_state.messages.append({"role": "assistant", "content": output.choices[0].message.content})

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    messages = st.session_state.get('messages', [])
    for message in messages:
        #roleãŒassistantã ã£ãŸã‚‰ã€assistantã®chat_messageã‚’ä½¿ã†
        if message['role'] == 'assistant':
            with st.chat_message('assistant'):
                st.markdown(message['content'])
        #roleãŒuserã ã£ãŸã‚‰ã€userã®chat_messageã‚’ä½¿ã†
        elif message['role'] == 'user':
            with st.chat_message('user'):
                st.markdown(message['content'])
        else: # ä½•ã‚‚å‡ºåŠ›ã—ãªã„  
            pass
    
if __name__ == '__main__':
    main()